{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The backpropagation algorithm is used in the classical feed-forward artificial neural network.\n",
    "\n",
    "It is the technique still used to train large deep learning networks.\n",
    "\n",
    "In this tutorial, you will discover how to implement the backpropagation algorithm for a neural network from scratch with Python.\n",
    "\n",
    "After completing this tutorial, you will know:\n",
    "\n",
    "How to forward-propagate an input to calculate an output.\n",
    "How to back-propagate error and train a network.\n",
    "How to apply the backpropagation algorithm to a real-world predictive modeling problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Description\n",
    "This section provides a brief introduction to the Backpropagation Algorithm and the Wheat Seeds dataset that we will be using in this tutorial.\n",
    "\n",
    "### Backpropagation Algorithm\n",
    "The Backpropagation algorithm is a supervised learning method for multilayer feed-forward networks from the field of Artificial Neural Networks.\n",
    "\n",
    "Feed-forward neural networks are inspired by the information processing of one or more neural cells, called a neuron. A neuron accepts input signals via its dendrites, which pass the electrical signal down to the cell body. The axon carries the signal out to synapses, which are the connections of a cell\u2019s axon to other cell\u2019s dendrites.\n",
    "\n",
    "The principle of the backpropagation approach is to model a given function by modifying internal weightings of input signals to produce an expected output signal. The system is trained using a supervised learning method, where the error between the system\u2019s output and a known expected output is presented to the system and used to modify its internal state.\n",
    "\n",
    "Technically, the backpropagation algorithm is a method for training the weights in a multilayer feed-forward neural network. As such, it requires a network structure to be defined of one or more layers where one layer is fully connected to the next layer. A standard network structure is one input layer, one hidden layer, and one output layer.\n",
    "\n",
    "Backpropagation can be used for both classification and regression problems, but we will focus on classification in this tutorial.\n",
    "\n",
    "In classification problems, best results are achieved when the network has one neuron in the output layer for each class value. For example, a 2-class or binary classification problem with the class values of A and B. These expected outputs would have to be transformed into binary vectors with one column for each class value. Such as [1, 0] and [0, 1] for A and B respectively. This is called a one hot encoding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tutorial\n",
    "This tutorial is broken down into 6 parts:\n",
    "\n",
    "- Initialize Network.\n",
    "- Forward Propagate.\n",
    "- Back Propagate Error.\n",
    "- Train Network.\n",
    "- Predict.\n",
    "- Seeds Dataset Case Study."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize a network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_network(n_inputs, n_hidden, n_outputs):\n",
    "    network = list()\n",
    "    hidden_layer = [\n",
    "        {\"weights\": [random() for i in range(n_inputs + 1)]} for i in range(n_hidden)\n",
    "    ]\n",
    "    network.append(hidden_layer)\n",
    "    output_layer = [\n",
    "        {\"weights\": [random() for i in range(n_hidden + 1)]} for i in range(n_outputs)\n",
    "    ]\n",
    "    network.append(output_layer)\n",
    "    return network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let\u2019s test out this function. Below is a complete example that creates a small network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import random, seed\n",
    "\n",
    "seed(1)\n",
    "network = initialize_network(2, 1, 2)\n",
    "for layer in network:\n",
    "    print(layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the example, you can see that the code prints out each layer one by one. You can see the hidden layer has one neuron with 2 input weights plus the bias. The output layer has 2 neurons, each with 1 weight plus the bias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward Propagate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can calculate an output from a neural network by propagating an input signal through each layer until the output layer outputs its values.\n",
    "\n",
    "We call this forward-propagation.\n",
    "\n",
    "It is the technique we will need to generate predictions during training that will need to be corrected, and it is the method we will need after the network is trained to make predictions on new data.\n",
    "\n",
    "We can break forward propagation down into three parts:\n",
    "\n",
    "- Neuron Activation.\n",
    "- Neuron Transfer.\n",
    "- Forward Propagation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neuron Activation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is to calculate the activation of one neuron given an input.\n",
    "\n",
    "The input could be a row from our training dataset, as in the case of the hidden layer. It may also be the outputs from each neuron in the hidden layer, in the case of the output layer.\n",
    "\n",
    "Neuron activation is calculated as the weighted sum of the inputs. Much like linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate neuron activation for an input\n",
    "def activate(weights, inputs):\n",
    "    activation = weights[-1]\n",
    "    for i in range(len(weights) - 1):\n",
    "        activation += weights[i] * inputs[i]\n",
    "    return activation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neuron Transfer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once a neuron is activated, we need to transfer the activation to see what the neuron output actually is.\n",
    "\n",
    "Different transfer functions can be used. It is traditional to use the sigmoid activation function, but you can also use the tanh (hyperbolic tangent) function to transfer outputs. More recently, the rectifier transfer function has been popular with large deep learning networks.\n",
    "\n",
    "The sigmoid activation function looks like an S shape, it\u2019s also called the logistic function. It can take any input value and produce a number between 0 and 1 on an S-curve. It is also a function of which we can easily calculate the derivative (slope) that we will need later when backpropagating error.\n",
    "\n",
    "We can transfer an activation function using the sigmoid function as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transfer neuron activation\n",
    "from math import exp\n",
    "\n",
    "\n",
    "def transfer(activation):\n",
    "    return 1.0 / (1.0 + exp(-activation))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Forward propagating an input is straightforward.\n",
    "\n",
    "We work through each layer of our network calculating the outputs for each neuron. All of the outputs from one layer become inputs to the neurons on the next layer.\n",
    "\n",
    "Below is a function named forward_propagate() that implements the forward propagation for a row of data from our dataset with our neural network.\n",
    "\n",
    "You can see that a neuron\u2019s output value is stored in the neuron with the name \u2018output\u2018. You can also see that we collect the outputs for a layer in an array named new_inputs that becomes the array inputs and is used as inputs for the following layer.\n",
    "\n",
    "The function returns the outputs from the last layer also called the output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward propagate input to a network output\n",
    "def forward_propagate(network, row):\n",
    "    inputs = row\n",
    "    for layer in network:\n",
    "        new_inputs = []\n",
    "        for neuron in layer:\n",
    "            activation = activate(neuron[\"weights\"], inputs)\n",
    "            neuron[\"output\"] = transfer(activation)\n",
    "            new_inputs.append(neuron[\"output\"])\n",
    "        inputs = new_inputs\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Back Propagate Error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The backpropagation algorithm is named for the way in which weights are trained.\n",
    "\n",
    "Error is calculated between the expected outputs and the outputs forward propagated from the network. These errors are then propagated backward through the network from the output layer to the hidden layer, assigning blame for the error and updating weights as they go.\n",
    "\n",
    "The math for backpropagating error is rooted in calculus, but we will remain high level in this section and focus on what is calculated and how rather than why the calculations take this particular form.\n",
    "\n",
    "This part is broken down into two sections.\n",
    "\n",
    "- Transfer Derivative.\n",
    "- Error Backpropagation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer Derivative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the derivative of an neuron output\n",
    "def transfer_derivative(output):\n",
    "    return output * (1.0 - output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is to calculate the error for each output neuron, this will give us our error signal (input) to propagate backwards through the network.\n",
    "\n",
    "The error for a given neuron can be calculated as follows:\n",
    "\n",
    "`error = (expected - output) * transfer_derivative(output)`\n",
    "\n",
    "Where expected is the expected output value for the neuron, output is the output value for the neuron and transfer_derivative() calculates the slope of the neuron\u2019s output value, as shown above.\n",
    "\n",
    "This error calculation is used for neurons in the output layer. The expected value is the class value itself. In the hidden layer, things are a little more complicated.\n",
    "\n",
    "The error signal for a neuron in the hidden layer is calculated as the weighted error of each neuron in the output layer. Think of the error traveling back along the weights of the output layer to the neurons in the hidden layer.\n",
    "\n",
    "The back-propagated error signal is accumulated and then used to determine the error for the neuron in the hidden layer, as follows:\n",
    "\n",
    "`error = (weight_k * error_j) * transfer_derivative(output)`\n",
    "\n",
    "Where error_j is the error signal from the jth neuron in the output layer, weight_k is the weight that connects the kth neuron to the current neuron and output is the output for the current neuron.\n",
    "\n",
    "Below is a function named backward_propagate_error() that implements this procedure.\n",
    "\n",
    "You can see that the error signal calculated for each neuron is stored with the name \u2018delta\u2019. You can see that the layers of the network are iterated in reverse order, starting at the output and working backwards. This ensures that the neurons in the output layer have \u2018delta\u2019 values calculated first that neurons in the hidden layer can use in the subsequent iteration. I chose the name \u2018delta\u2019 to reflect the change the error implies on the neuron (e.g. the weight delta).\n",
    "\n",
    "You can see that the error signal for neurons in the hidden layer is accumulated from neurons in the output layer where the hidden neuron number j is also the index of the neuron\u2019s weight in the output layer neuron[\u2018weights\u2019][j]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backpropagate error and store in neurons\n",
    "def backward_propagate_error(network, expected):\n",
    "    for i in reversed(range(len(network))):\n",
    "        layer = network[i]\n",
    "        errors = list()\n",
    "        if i != len(network) - 1:\n",
    "            for j in range(len(layer)):\n",
    "                error = 0.0\n",
    "                for neuron in network[i + 1]:\n",
    "                    error += neuron[\"weights\"][j] * neuron[\"delta\"]\n",
    "                errors.append(error)\n",
    "        else:\n",
    "            for j in range(len(layer)):\n",
    "                neuron = layer[j]\n",
    "                errors.append(expected[j] - neuron[\"output\"])\n",
    "        for j in range(len(layer)):\n",
    "            neuron = layer[j]\n",
    "            neuron[\"delta\"] = errors[j] * transfer_derivative(neuron[\"output\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test backpropagation of error\n",
    "network = [\n",
    "    [\n",
    "        {\n",
    "            \"output\": 0.7105668883115941,\n",
    "            \"weights\": [0.13436424411240122, 0.8474337369372327, 0.763774618976614],\n",
    "        }\n",
    "    ],\n",
    "    [\n",
    "        {\n",
    "            \"output\": 0.6213859615555266,\n",
    "            \"weights\": [0.2550690257394217, 0.49543508709194095],\n",
    "        },\n",
    "        {\n",
    "            \"output\": 0.6573693455986976,\n",
    "            \"weights\": [0.4494910647887381, 0.651592972722763],\n",
    "        },\n",
    "    ],\n",
    "]\n",
    "expected = [0, 1]\n",
    "backward_propagate_error(network, expected)\n",
    "for layer in network:\n",
    "    print(layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The network is trained using stochastic gradient descent.\n",
    "\n",
    "This involves multiple iterations of exposing a training dataset to the network and for each row of data forward propagating the inputs, backpropagating the error and updating the network weights.\n",
    "\n",
    "This part is broken down into two sections:\n",
    "\n",
    "- Update Weights.\n",
    "- Train Network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update Weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Network weights are updated as follows:\n",
    "\n",
    "`weight = weight + learning_rate * error * input`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update network weights with error\n",
    "def update_weights(network, row, l_rate):\n",
    "    for i in range(len(network)):\n",
    "        inputs = row[:-1]\n",
    "        if i != 0:\n",
    "            inputs = [neuron[\"output\"] for neuron in network[i - 1]]\n",
    "        for neuron in network[i]:\n",
    "            for j in range(len(inputs)):\n",
    "                neuron[\"weights\"][j] += l_rate * neuron[\"delta\"] * inputs[j]\n",
    "            neuron[\"weights\"][-1] += l_rate * neuron[\"delta\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a network for a fixed number of epochs\n",
    "def train_network(network, train, l_rate, n_epoch, n_outputs):\n",
    "    for epoch in range(n_epoch):\n",
    "        sum_error = 0\n",
    "        for row in train:\n",
    "            outputs = forward_propagate(network, row)\n",
    "            expected = [0 for i in range(n_outputs)]\n",
    "            expected[row[-1]] = 1\n",
    "            sum_error += sum(\n",
    "                [(expected[i] - outputs[i]) ** 2 for i in range(len(expected))]\n",
    "            )\n",
    "            backward_propagate_error(network, expected)\n",
    "            update_weights(network, row, l_rate)\n",
    "        print(\">epoch=%d, lrate=%.3f, error=%.3f\" % (epoch, l_rate, sum_error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test training backprop algorithm\n",
    "seed(1)\n",
    "dataset = [\n",
    "    [2.7810836, 2.550537003, 0],\n",
    "    [1.465489372, 2.362125076, 0],\n",
    "    [3.396561688, 4.400293529, 0],\n",
    "    [1.38807019, 1.850220317, 0],\n",
    "    [3.06407232, 3.005305973, 0],\n",
    "    [7.627531214, 2.759262235, 1],\n",
    "    [5.332441248, 2.088626775, 1],\n",
    "    [6.922596716, 1.77106367, 1],\n",
    "    [8.675418651, -0.242068655, 1],\n",
    "    [7.673756466, 3.508563011, 1],\n",
    "]\n",
    "n_inputs = len(dataset[0]) - 1\n",
    "n_outputs = len(set([row[-1] for row in dataset]))\n",
    "network = initialize_network(n_inputs, 2, n_outputs)\n",
    "train_network(network, dataset, 0.5, 20, n_outputs)\n",
    "for layer in network:\n",
    "    print(layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a prediction with a network\n",
    "def predict(network, row):\n",
    "    outputs = forward_propagate(network, row)\n",
    "    return outputs.index(max(outputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test making predictions with the network\n",
    "dataset = [\n",
    "    [2.7810836, 2.550537003, 0],\n",
    "    [1.465489372, 2.362125076, 0],\n",
    "    [3.396561688, 4.400293529, 0],\n",
    "    [1.38807019, 1.850220317, 0],\n",
    "    [3.06407232, 3.005305973, 0],\n",
    "    [7.627531214, 2.759262235, 1],\n",
    "    [5.332441248, 2.088626775, 1],\n",
    "    [6.922596716, 1.77106367, 1],\n",
    "    [8.675418651, -0.242068655, 1],\n",
    "    [7.673756466, 3.508563011, 1],\n",
    "]\n",
    "network = [\n",
    "    [\n",
    "        {\"weights\": [-1.482313569067226, 1.8308790073202204, 1.078381922048799]},\n",
    "        {\"weights\": [0.23244990332399884, 0.3621998343835864, 0.40289821191094327]},\n",
    "    ],\n",
    "    [\n",
    "        {\"weights\": [2.5001872433501404, 0.7887233511355132, -1.1026649757805829]},\n",
    "        {\"weights\": [-2.429350576245497, 0.8357651039198697, 1.0699217181280656]},\n",
    "    ],\n",
    "]\n",
    "for row in dataset:\n",
    "    prediction = predict(network, row)\n",
    "    print(\"Expected=%d, Got=%d\" % (row[-1], prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a CSV file\n",
    "def load_csv(filename):\n",
    "    dataset = list()\n",
    "    with open(filename, \"r\") as file:\n",
    "        csv_reader = reader(file)\n",
    "        for row in csv_reader:\n",
    "            if not row:\n",
    "                continue\n",
    "            dataset.append(row)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Test Backprop on Seeds dataset\n",
    "seed(1)\n",
    "\n",
    "# load and prepare data\n",
    "filename = \"data/seeds_dataset.txt\"\n",
    "\n",
    "dataset = pd.read_csv(filename, sep=\"\\t\")\n",
    "\n",
    "for i in range(len(dataset[0]) - 1):\n",
    "    str_column_to_float(dataset, i)\n",
    "\n",
    "# convert class column to integers\n",
    "str_column_to_int(dataset, len(dataset[0]) - 1)\n",
    "\n",
    "# normalize input variables\n",
    "minmax = dataset_minmax(dataset)\n",
    "normalize_dataset(dataset, minmax)\n",
    "\n",
    "# evaluate algorithm\n",
    "n_folds = 5\n",
    "l_rate = 0.3\n",
    "n_epoch = 500\n",
    "n_hidden = 5\n",
    "scores = evaluate_algorithm(\n",
    "    dataset, back_propagation, n_folds, l_rate, n_epoch, n_hidden\n",
    ")\n",
    "\n",
    "print(\"Scores: %s\" % scores)\n",
    "print(\"Mean Accuracy: %.3f%%\" % (sum(scores) / float(len(scores))))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
